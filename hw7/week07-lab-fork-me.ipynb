{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install timm\nfrom IPython.display import Image as ImageIpython\nfrom IPython.core.display import HTML","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:50:22.366391Z","iopub.execute_input":"2021-10-12T00:50:22.366821Z","iopub.status.idle":"2021-10-12T00:50:30.351449Z","shell.execute_reply.started":"2021-10-12T00:50:22.366736Z","shell.execute_reply":"2021-10-12T00:50:30.350428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Table of contents\n* [Initial Setup](#section-one)\n* [LR Schedulers & Question](#section-two)\n* [Mixup & Question](#section-three)\n* [Architectures & Question](#section-four)\n* [Mixed Precision & Question](#section-five)\n* [Label Smoothing & Question](#section-six)","metadata":{}},{"cell_type":"code","source":"print(\"Imagenet improvement on Resnet-50 using tricks\")\nImageIpython(url= \"https://miro.medium.com/max/890/1*4y2Rdl0i-kvNh2SCMZZBuw.png\")","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:50:30.353083Z","iopub.execute_input":"2021-10-12T00:50:30.353416Z","iopub.status.idle":"2021-10-12T00:50:30.365061Z","shell.execute_reply.started":"2021-10-12T00:50:30.35338Z","shell.execute_reply":"2021-10-12T00:50:30.364137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n## Initial Set up - just run through these steps","metadata":{}},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm_notebook as tqdm\n#import tqdm.notebook import tqdm\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport torch\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import lr_scheduler\nimport timm\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:50:30.367354Z","iopub.execute_input":"2021-10-12T00:50:30.368022Z","iopub.status.idle":"2021-10-12T00:50:33.980296Z","shell.execute_reply.started":"2021-10-12T00:50:30.367985Z","shell.execute_reply":"2021-10-12T00:50:33.979348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -l ../input/midsw251birds","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:50:33.981799Z","iopub.execute_input":"2021-10-12T00:50:33.982142Z","iopub.status.idle":"2021-10-12T00:50:34.669483Z","shell.execute_reply.started":"2021-10-12T00:50:33.982101Z","shell.execute_reply":"2021-10-12T00:50:34.668536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class args:\n    lr = 0.0001\n    epochs = 5\n    batch_size = 32\n    num_workers = 8\n    folds = 5","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:50:34.672675Z","iopub.execute_input":"2021-10-12T00:50:34.672963Z","iopub.status.idle":"2021-10-12T00:50:34.678689Z","shell.execute_reply.started":"2021-10-12T00:50:34.67293Z","shell.execute_reply":"2021-10-12T00:50:34.676862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alldf = pd.read_csv('../input/midsw251birds/train.csv')\n# Split the training dataset into a training and a validation\nvaldf = alldf[::args.folds]\ntrndf = alldf[~alldf.filename.isin(valdf.filename)]\n# Load our test data\ntstdf = pd.read_csv('../input/midsw251birds/test.csv')\nmetadf = pd.read_csv('../input/midsw251birds/metadata.csv')\nmetadf = metadf.set_index('label')\nprint(f'File shapes -- train : {trndf.shape}, valid : {valdf.shape}, test : {tstdf.shape}')\ntrndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:51:32.53182Z","iopub.execute_input":"2021-10-12T00:51:32.532207Z","iopub.status.idle":"2021-10-12T00:51:32.629376Z","shell.execute_reply.started":"2021-10-12T00:51:32.532173Z","shell.execute_reply":"2021-10-12T00:51:32.62848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgnetmeans = [0.22363983, 0.18190407, 0.2523437 ]\nimgnetstds = [0.32451536, 0.2956294,  0.31335256]\n#Â Using albumentations, check some examples here : https://albumentations.readthedocs.io/en/latest/examples.html \ndef trntransforms():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Transpose(p=0.5),\n        ToTensorV2(),\n        ])\n\ndef tsttransforms():\n    return A.Compose([\n        ToTensorV2(),\n    ])\n\nclass BirdDataset(Dataset):\n    def __init__(self, df, mode, transform=None):\n        self.data = df\n        self.img_dir = '../input/midsw251birds/'\n        self.transform = transform\n        self.mode = mode\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \n        fname = self.data.iloc[idx]['filename']\n        image = cv2.imread(f'{self.img_dir}/{fname}')\n        if self.transform is not None:\n            image = self.transform(image = image)['image']\n        image = image.float() / 255.\n        label = -1 if self.mode=='test' else self.data.iloc[idx]['label']\n        \n        return image, label","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:51:35.275776Z","iopub.execute_input":"2021-10-12T00:51:35.276157Z","iopub.status.idle":"2021-10-12T00:51:35.285308Z","shell.execute_reply.started":"2021-10-12T00:51:35.276123Z","shell.execute_reply":"2021-10-12T00:51:35.284435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define our dataset\ntrndataset = BirdDataset(trndf, 'train', trntransforms())\nvaldataset = BirdDataset(valdf, 'valid', tsttransforms())\ntstdataset = BirdDataset(tstdf, 'test', tsttransforms())\nloaderargs = {'num_workers' : args.num_workers, 'batch_size':args.batch_size, 'pin_memory': False, 'drop_last': False}\ntrnloader = DataLoader(trndataset, shuffle = True, **loaderargs)\nvalloader = DataLoader(valdataset, shuffle = False, **loaderargs)\ntstloader = DataLoader(tstdataset, shuffle = False, **loaderargs)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:51:37.130867Z","iopub.execute_input":"2021-10-12T00:51:37.131225Z","iopub.status.idle":"2021-10-12T00:51:37.139848Z","shell.execute_reply.started":"2021-10-12T00:51:37.131191Z","shell.execute_reply":"2021-10-12T00:51:37.138989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creates efficientnet-b0 architecture\ndevice = torch.device(\"cuda:0\")\nmodel = timm.create_model('efficientnet_b2', pretrained = True)\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\ncriterion = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:51:41.100384Z","iopub.execute_input":"2021-10-12T00:51:41.100716Z","iopub.status.idle":"2021-10-12T00:51:47.355815Z","shell.execute_reply.started":"2021-10-12T00:51:41.100684Z","shell.execute_reply":"2021-10-12T00:51:47.354982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# LR Schedulers","metadata":{}},{"cell_type":"code","source":"# Create a scheduler which will warmup and cooldown over 20 epochs.\nfrom timm.scheduler.cosine_lr import CosineLRScheduler\n\nn_epochs = 20\nn_warmup_epochs = 2\nn_steps = len(trnloader)\n\nscheduler = CosineLRScheduler(\n            optimizer,\n            t_initial= n_steps * n_epochs + 1,\n            lr_min=0.00001,\n            warmup_lr_init=0.00001,\n            warmup_t= n_steps * n_warmup_epochs + 1)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:51:47.359127Z","iopub.execute_input":"2021-10-12T00:51:47.359385Z","iopub.status.idle":"2021-10-12T00:51:47.366464Z","shell.execute_reply.started":"2021-10-12T00:51:47.359359Z","shell.execute_reply":"2021-10-12T00:51:47.36567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's visualise how this changes the LR\nlrls = []\nglobal_step = 0\nfor epoch in range(n_epochs):\n    for step in range(len(trnloader)):\n        #train_step(...)\n        scheduler.step(global_step)\n        global_step+=1\n        lrls.append(optimizer.param_groups[0]['lr']) \n    #validate_epoch(...)\n# Plot\nax = pd.Series(lrls).plot(logy=True, figsize = (10, 4))\nfor i in range(0,n_epochs*n_steps+1,n_steps) : ax.axvline(i, linewidth=0.2, color='r', linestyle='--')\nax.set_xlabel(\"steps\")\nax.set_ylabel(\"LR (log scale)\")","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:51:47.70471Z","iopub.execute_input":"2021-10-12T00:51:47.705061Z","iopub.status.idle":"2021-10-12T00:51:48.453485Z","shell.execute_reply.started":"2021-10-12T00:51:47.705026Z","shell.execute_reply":"2021-10-12T00:51:48.452625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Cosine Annealing is a type of learning rate schedule that has the effect of starting with a large learning rate that is relatively rapidly decreased to a minimum value before being increased rapidly again. The resetting of the learning rate acts like a simulated restart of the learning process and the re-use of good weights as the starting point of the restart is referred to as a \"warm restart\" in contrast to a \"cold restart\" where a new set of small random numbers may be used as a starting point.","metadata":{}},{"cell_type":"code","source":"lrls = []\nglobal_step = 0\nfor epoch in range(n_epochs * 6):\n    for step in range(len(trnloader)):\n        #train_step(...)\n        scheduler.step(global_step)\n        global_step+=1\n        lrls.append(optimizer.param_groups[0]['lr']) \n    #validate_epoch(...)\n# Plot\nax = pd.Series(lrls).plot(logy=True, figsize = (10, 4))\nfor i in range(0,n_epochs * 6 *n_steps+1,n_steps) : ax.axvline(i, linewidth=0.2, color='r', linestyle='--')\nax.set_xlabel(\"steps\")\nax.set_ylabel(\"LR (log scale)\")","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:51:49.931738Z","iopub.execute_input":"2021-10-12T00:51:49.932087Z","iopub.status.idle":"2021-10-12T00:51:51.136286Z","shell.execute_reply.started":"2021-10-12T00:51:49.932054Z","shell.execute_reply":"2021-10-12T00:51:51.135488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"#subsection-two-one\"></a>\n### Question : Can you implement a cosine learning rate schedule which has 4 epochs warmup and reaches a minimum at 15 epochs, using the n_steps from `trnloader`. ","metadata":{}},{"cell_type":"code","source":"n_epochs = 15\nn_warmup_epochs = 4\nn_steps = len(trnloader)\n\nscheduler = CosineLRScheduler(\n            optimizer,\n            t_initial= n_steps * n_epochs + 1,\n            lr_min=0.00001,\n            warmup_lr_init=0.00001,\n            warmup_t= n_steps * n_warmup_epochs + 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lrls = []\nglobal_step = 0\nfor epoch in range(n_epochs * 6):\n    for step in range(len(trnloader)):\n        #train_step(...)\n        scheduler.step(global_step)\n        global_step+=1\n        lrls.append(optimizer.param_groups[0]['lr']) \n    #validate_epoch(...)\n# Plot\nax = pd.Series(lrls).plot(logy=True, figsize = (10, 4))\nfor i in range(0,n_epochs * 6 *n_steps+1,n_steps) : ax.axvline(i, linewidth=0.2, color='r', linestyle='--')\nax.set_xlabel(\"steps\")\nax.set_ylabel(\"LR (log scale)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n## Mixup","metadata":{}},{"cell_type":"code","source":"ImageIpython(url= \"https://forums.fast.ai/uploads/default/original/3X/4/b/4b00023c65aa58fbe58b02271de08949e53c64b9.png\")","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:52:10.694508Z","iopub.execute_input":"2021-10-12T00:52:10.694873Z","iopub.status.idle":"2021-10-12T00:52:10.700597Z","shell.execute_reply.started":"2021-10-12T00:52:10.694829Z","shell.execute_reply":"2021-10-12T00:52:10.699478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets try this very simply. \nimg1 = cv2.imread(f'../input/midsw251birds/{trndf.iloc[0].filename}')\nimg2 = cv2.imread(f'../input/midsw251birds/{trndf.iloc[1].filename}')\nmixup_alpha = 0.6\nimg_mixed = (img1 * mixup_alpha + img2 * (1-mixup_alpha)).astype(np.uint8)\nImage.fromarray(np.concatenate([img1,img2,img_mixed], 1))","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:52:14.157346Z","iopub.execute_input":"2021-10-12T00:52:14.157668Z","iopub.status.idle":"2021-10-12T00:52:14.290504Z","shell.execute_reply.started":"2021-10-12T00:52:14.157635Z","shell.execute_reply":"2021-10-12T00:52:14.28958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beta = 1.0\n\nfor step, batch in enumerate(trnloader):\n    if step > 20: \n        break\n    inputs = batch[0].to(device, dtype=torch.float)\n    labels = batch[1].to(device).long()\n    \n    # Get out a random value form a distribution    \n    lam = np.random.beta(beta, beta)\n    rand_index = torch.randperm(inputs.size()[0]).to(device) # make an index which reorders the batch\n    \n    # Reorder the labels\n    labels_a = labels\n    labels_b = labels[rand_index]\n    \n    # Partially mixup up the batch\n    inputs_mixed = lam * inputs + (1 - lam) * inputs[rand_index]\n    \n    optimizer.zero_grad()\n    output = model(inputs_mixed)\n    \n    # Partial loss against original labels, partial loss against mixed up labels\n    loss = criterion(output, labels_a) * lam + criterion(output, labels_b) * (1. - lam)\n    loss.backward()\n    optimizer.step()\n    \n    # Note, do not mixup your validation or test data !! Just train, make the model sweat....  ","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:52:19.850495Z","iopub.execute_input":"2021-10-12T00:52:19.850859Z","iopub.status.idle":"2021-10-12T00:52:27.039276Z","shell.execute_reply.started":"2021-10-12T00:52:19.850825Z","shell.execute_reply":"2021-10-12T00:52:27.03821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Questions \nCan you plot the different distributions of `lam`. (Something like ... `pd.Series([np.random.beta(beta, beta) for i in range(1000)]).hist(bins=100)`).   \nWhy would we us larger or smaller values of `lam` ?   \nCan you understand how the loss is calculated above, and why ?   \nCan you guess how you would implement cutmix in the example above ?","metadata":{}},{"cell_type":"code","source":"a = pd.Series([np.random.beta(beta, beta) for i in range(1000)]).hist(bins=100)\na.set_xlabel('x')\na.set_ylabel('y')\na.plot()\n# larger values increase generalizeability, trade off would be decrease accuracy\n# smaller values increase accuracy, trade off would be overfitting","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:52:27.041098Z","iopub.execute_input":"2021-10-12T00:52:27.041453Z","iopub.status.idle":"2021-10-12T00:52:27.348317Z","shell.execute_reply.started":"2021-10-12T00:52:27.041414Z","shell.execute_reply":"2021-10-12T00:52:27.347306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# Different Architectures","metadata":{}},{"cell_type":"markdown","source":"#### Above we chose to work with efficientnet-b0, can you initialise a pretrained `mixnet-xl` model ? Tip, check the results table [here](https://github.com/rwightman/pytorch-image-models/blob/master/results/results-imagenet.csv)","metadata":{}},{"cell_type":"code","source":"# model = timm.create_model('efficientnet_b2', pretrained = True)\nmodel = timm.create_model('mixnet_xl', pretrained = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Our model still has 1000 classes outputted. How would you initialise it with 10 classes in one line of code. Tip, try checking https://fastai.github.io/timmdocs/","metadata":{}},{"cell_type":"code","source":"model2 = timm.create_model('mixnet_xl', pretrained = True, num_classes=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# Mixed Precision Training","metadata":{}},{"cell_type":"code","source":"ImageIpython(url= \"https://developer-blogs.nvidia.com/wp-content/uploads/2019/10/Screen-Shot-2019-10-18-at-7.31.09-AM-624x328.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-12T00:52:35.995906Z","iopub.execute_input":"2021-10-12T00:52:35.996258Z","iopub.status.idle":"2021-10-12T00:52:36.00157Z","shell.execute_reply.started":"2021-10-12T00:52:35.996227Z","shell.execute_reply":"2021-10-12T00:52:36.000396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Can you implement mixed precision in the below training loop. Tip : Check [this](https://pytorch.org/docs/stable/notes/amp_examples.html#typical-mixed-precision-training) example. You will need to comment out some of the below lines, its not too tough.\n### See how large you can make your batchsize with mixed precision, and without mixed precision (you may need ot restart the kernels a few times.)","metadata":{}},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nmodel.to(device)\nmodel.train()\n\nfor step, batch in enumerate(trnloader):\n    if step > 20:\n        break\n    \n    optimizer.zero_grad()\n    \n    inputs = batch[0].to(device, dtype=torch.float)\n    labels = batch[1].to(device).long()\n    \n    with autocast():\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n    \n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:52:37.864938Z","iopub.execute_input":"2021-10-12T00:52:37.865268Z","iopub.status.idle":"2021-10-12T00:52:51.588191Z","shell.execute_reply.started":"2021-10-12T00:52:37.865237Z","shell.execute_reply":"2021-10-12T00:52:51.587113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n# Label Smoothing\nThe generalization and learning speed of a multi-class neural network can often\nbe significantly improved by using soft targets that are a weighted average of the\nhard targets and the uniform distribution over labels. Smoothing the labels in this\nway prevents the network from becoming over-confident and label smoothing has\nbeen used in many state-of-the-art models,","metadata":{}},{"cell_type":"code","source":"ImageIpython(url= \"https://paperswithcode.com/media/methods/image3_1_oTiwmLN.png\")","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:53:06.640031Z","iopub.execute_input":"2021-10-12T00:53:06.640368Z","iopub.status.idle":"2021-10-12T00:53:06.645876Z","shell.execute_reply.started":"2021-10-12T00:53:06.640337Z","shell.execute_reply":"2021-10-12T00:53:06.644981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Can you implement label smoothing in the below training loop. Tip : Check [this](https://github.com/pytorch/pytorch/issues/7455#issuecomment-513062631) or [this](https://github.com/pytorch/pytorch/issues/7455#issuecomment-759175034) example. ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.loss import _WeightedLoss\n\n\nclass LabelSmoothCrossEntropyLoss(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth_one_hot(targets: torch.Tensor, n_classes: int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = torch.empty(size=(targets.size(0), n_classes),\n                                  device=targets.device) \\\n                .fill_(smoothing / (n_classes - 1)) \\\n                .scatter_(1, targets.data.unsqueeze(1), 1. - smoothing)\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = LabelSmoothCrossEntropyLoss._smooth_one_hot(targets, inputs.size(-1),\n                                                              self.smoothing)\n        lsm = F.log_softmax(inputs, -1)\n\n        if self.weight is not None:\n            lsm = lsm * self.weight.unsqueeze(0)\n\n        loss = -(targets * lsm).sum(-1)\n\n        if self.reduction == 'sum':\n            loss = loss.sum()\n        elif self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:53:17.093597Z","iopub.execute_input":"2021-10-12T00:53:17.093937Z","iopub.status.idle":"2021-10-12T00:53:17.103749Z","shell.execute_reply.started":"2021-10-12T00:53:17.093885Z","shell.execute_reply":"2021-10-12T00:53:17.102924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler\n\nmodel.to(device)\nmodel.train()\ncriterion = LabelSmoothCrossEntropyLoss(smoothing=0.1)\n\nfor step, batch in enumerate(trnloader):\n    if step > 20:\n        break\n    inputs = batch[0].to(device, dtype=torch.float)\n    labels = batch[1].to(device).long()\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2021-10-12T00:53:21.993384Z","iopub.execute_input":"2021-10-12T00:53:21.993707Z","iopub.status.idle":"2021-10-12T00:53:28.089215Z","shell.execute_reply.started":"2021-10-12T00:53:21.993678Z","shell.execute_reply":"2021-10-12T00:53:28.088092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}